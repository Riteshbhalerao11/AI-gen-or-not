{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d38e9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 05:41:56.583207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLNetTokenizer , TFXLNetModel\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Attention, Input, Dense, LSTM, Embedding, Bidirectional, Dropout, TimeDistributed, concatenate, MaxPooling1D, Activation, Add, Flatten, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705c6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Datasets/test_essays.csv')\n",
    "sub = pd.read_csv('Datasets/sample_submission.csv')\n",
    "org_train = pd.read_csv('Datasets/train_essays.csv')\n",
    "train = pd.read_csv(\"Datasets/train_v2_drcat_02.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1f399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates(subset=['text'])\n",
    "train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1915e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_train.rename(columns = {'generated':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba952fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train[['text','label']], org_train[['text','label']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85b41251",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_data.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6a30d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "labels = train_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51628935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁phones', '▁modern', '▁humans', '▁today', '▁are', '▁always', '▁on', '▁their', '▁phone', '.', '▁they', '▁are', '▁always', '▁on', '▁their', '▁phone', '▁more', '▁than', '▁5', '▁hours', '▁a', '▁day', '▁no', '▁stop', '▁', '.', 'all', '▁they', '▁do', '▁is', '▁text', '▁back', '▁and', '▁forward', '▁and', '▁just', '▁have', '▁group', '▁chat', 's', '▁on', '▁social', '▁media', '.', '▁they', '▁even', '▁do', '▁it', '▁while', '▁driving', '.', '▁they', '▁are', '▁some', '▁really', '▁bad', '▁consequences', '▁when', '▁stuff', '▁happens', '▁when', '▁it', '▁comes', '▁to', '▁a', '▁phone', '.', '▁some', '▁certain', '▁areas', '▁in', '▁the', '▁united', '▁states', '▁ban', '▁phones', '▁from', '▁class', '▁rooms', '▁just', '▁because', '▁of', '▁it', '.', '▁when', '▁people', '▁have', '▁phones', ',', '▁they', '▁know', '▁about', '▁certain', '▁apps', '▁that', '▁they', '▁have', '▁', '.', 'app', 's', '▁like', '▁face', 'book', '▁twitter', '▁in', 'sta', 'gram', '▁and', '▁snap', 'cha', 't', '.', '▁so', '▁like', '▁if', '▁a', '▁friend', '▁moves', '▁away', '▁and', '▁you', '▁want', '▁to', '▁be', '▁in', '▁contact', '▁you', '▁can', '▁still', '▁be', '▁in', '▁contact', '▁by', '▁posting', '▁videos', '▁or', '▁text', '▁messages', '.', '▁people', '▁always', '▁have', '▁different', '▁ways', '▁how', '▁to', '▁communicate', '▁with', '▁a', '▁phone', '.', '▁phones', '▁have', '▁changed', '▁due', '▁to', '▁our', '▁generation', '.', '▁driving', '▁is', '▁one', '▁of', '▁the', '▁way', '▁how', '▁to', '▁get', '▁around', '.', '▁people', '▁always', '▁be', '▁on', '▁their', '▁phones', '▁while', '▁doing', '▁it', '.', '▁which', '▁can', '▁cause', '▁serious', '▁problems', '.', '▁that', \"'\", 's', '▁why', '▁there', \"'\", 's', '▁a', '▁thing', '▁that', \"'\", 's', '▁called', '▁no', '▁text', 'ing', '▁while', '▁driving', '.', '▁that', \"'\", 's', '▁a', '▁really', '▁important', '▁thing', '▁to', '▁remember', '.', '▁some', '▁people', '▁still', '▁do', '▁it', '▁because', '▁they', '▁think', '▁it', \"'\", 's', '▁stupid', '.', '▁no', '▁matter', '▁what', '▁they', '▁do', '▁they', '▁still', '▁have', '▁to', '▁obey', '▁it', '▁because', '▁that', \"'\", 's', '▁the', '▁only', '▁way', '▁how', '▁did', '▁he', '▁save', '.', '▁sometimes', '▁on', '▁the', '▁news', '▁there', '▁is', '▁either', '▁an', '▁accident', '▁or', '▁a', '▁suicide', '.', '▁it', '▁might', '▁involve', '▁someone', '▁not', '▁looking', '▁where', '▁they', \"'\", 're', '▁going', '▁or', '▁tweet', '▁that', '▁someone', '▁sent', '.', '▁it', '▁either', '▁injury', '▁or', '▁death', '.', '▁if', '▁a', '▁mysterious', '▁number', '▁says', '▁', 'i', \"'\", 'm', '▁going', '▁to', '▁kill', '▁you', '▁and', '▁they', '▁know', '▁where', '▁you', '▁live', '▁but', '▁you', '▁don', \"'\", 't', '▁know', '▁the', '▁person', \"'\", 's', '▁contact', '▁', ',', 'it', '▁makes', '▁you', '▁puzzled', '▁and', '▁make', '▁you', '▁start', '▁to', '▁freak', '▁out', '.', '▁which', '▁can', '▁end', '▁up', '▁really', '▁badly', '.', '▁phones', '▁are', '▁fine', '▁to', '▁use', '▁and', '▁it', \"'\", 's', '▁also', '▁the', '▁best', '▁way', '▁to', '▁come', '▁over', '▁help', '.', '▁if', '▁you', '▁go', '▁through', '▁a', '▁problem', '▁and', '▁you', '▁can', \"'\", 't', '▁find', '▁help', '▁you', '▁', ',', 'al', 'ways', '▁have', '▁a', '▁phone', '▁there', '▁with', '▁you', '.', '▁even', '▁though', '▁phones', '▁are', '▁used', '▁almost', '▁every', '▁day', '▁as', '▁long', '▁as', '▁you', \"'\", 're', '▁safe', '▁it', '▁would', '▁come', '▁into', '▁use', '▁if', '▁you', '▁get', '▁into', '▁trouble', '.', '▁make', '▁sure', '▁you', '▁do', '▁not', '▁be', '▁like', '▁this', '▁phone', '▁while', '▁you', \"'\", 're', '▁in', '▁the', '▁middle', '▁of', '▁driving', '.', '▁the', '▁news', '▁always', '▁updated', '▁when', '▁people', '▁do', '▁something', '▁stupid', '▁around', '▁that', '▁involves', '▁their', '▁phones', '.', '▁the', '▁safest', '▁way', '▁is', '▁the', '▁best', '▁way', '▁to', '▁stay', '▁safe', '.', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a477c5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478.03323530683735\n"
     ]
    }
   ],
   "source": [
    "avg_word_approx = 0\n",
    "for text in tokenized_texts:\n",
    "    avg_word_approx+= len(text)\n",
    "print(avg_word_approx/len(tokenized_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "970159d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a4385ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f222247",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da26892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ceb8963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and temporary set\n",
    "train_inputs, temp_inputs, train_labels, temp_labels = train_test_split(input_ids, labels, \n",
    "                                                                         random_state=42, test_size=0.2)\n",
    "\n",
    "# Split the temporary set into test and dev sets\n",
    "validation_inputs, test_inputs, validation_labels, test_labels = train_test_split(temp_inputs, temp_labels,\n",
    "                                                                                  random_state=42, test_size=0.5)\n",
    "\n",
    "del temp_inputs\n",
    "del temp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94545e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and temporary set\n",
    "train_mask, temp_mask_1 = train_test_split(attention_masks,random_state=42, test_size=0.2)\n",
    "\n",
    "# Split the temporary set into test and dev sets\n",
    "validation_mask, test_mask = train_test_split(temp_mask_1, random_state=42, test_size=0.5)\n",
    "\n",
    "del temp_mask_1\n",
    "# del temp_mask_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8e118ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    input_ids = Input(shape=(256,), dtype='int64')\n",
    "    attention_mask = Input(shape=(256,), dtype='int64')\n",
    "\n",
    "    print('Loading XLNetModel')\n",
    "    xlnetModel = TFXLNetModel.from_pretrained('xlnet-base-cased')\n",
    "    conv1D_shared = Conv1D(64, kernel_size=(7), strides=(2))\n",
    "    batchN = BatchNormalization()\n",
    "    activa = Activation('relu')\n",
    "#     attent = Attention(use_scale=True)\n",
    "    \n",
    "    xlnetout = xlnetModel.transformer({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\n",
    "    x = conv1D_shared(xlnetout.last_hidden_state)\n",
    "    x = batchN(x)\n",
    "    x = activa(x)\n",
    "#     x = attent([x, x])\n",
    "    x = MaxPooling1D((3), strides=(2))(x)\n",
    "    # x = Dense(1, activation=\"relu\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask,], outputs=x)\n",
    "    model.summary()\n",
    "    adam = Adam(learning_rate=0.00001)\n",
    "    model.compile(optimizer=adam, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70a116a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Include the epoch in the file name (uses `str.format`)\n",
    "#     checkpoint_path = \"./tripletloss/training_model/cp-{epoch:04d}\"\n",
    "    checkpoint_path = \"./model/training_model/cp-{epoch:04d}\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    # Create a callback that saves the model's weights every epoch\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_freq=\"epoch\")\n",
    "\n",
    "#     log_dir = \"./tripletloss/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir = \"./model/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)\n",
    "    # %load_ext tensorboard\n",
    "    # %tensorboard --logdir logs/fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb9bcce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading XLNetModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at xlnet-base-cased were not used when initializing TFXLNetModel: ['lm_loss']\n",
      "- This IS expected if you are initializing TFXLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFXLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFXLNetModel were initialized from the model checkpoint at xlnet-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFXLNetModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " transformer (TFXLNetMainLa  TFXLNetModelOutput(last_hi   1167183   ['input_6[0][0]',             \n",
      " yer)                        dden_state=(None, 256, 768   36         'input_5[0][0]']             \n",
      "                             ),                                                                   \n",
      "                              mems=((256, None, 768),                                             \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768)),                                                  \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 125, 64)              344128    ['transformer[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 125, 64)              256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 125, 64)              0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 62, 64)               0         ['activation_2[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 3968)                 0         ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    3969      ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 117066689 (446.57 MB)\n",
      "Trainable params: 117066561 (446.57 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6ee808d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tfxl_net_model_2/transformer/mask_emb:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._0/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._1/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._2/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._3/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._4/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._5/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._6/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._7/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._8/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._9/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._10/rel_attn/seg_embed:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/r_s_bias:0', 'tfxl_net_model_2/transformer/layer_._11/rel_attn/seg_embed:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "2313/2313 [==============================] - ETA: 0s - loss: 0.0521 - acc: 0.9816 - precision_1: 0.9774 - recall_1: 0.9736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 22:30:22.855447: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5898240000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to ./model/training_model/cp-0001\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3380d3d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33817250>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3381c0d0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33819f40>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33822dc0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3382bc40>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33833af0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3383b970>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337c37f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337cc670>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337d44f0>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337de370>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: ./model/training_model/cp-0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/training_model/cp-0001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313/2313 [==============================] - 3216s 1s/step - loss: 0.0521 - acc: 0.9816 - precision_1: 0.9774 - recall_1: 0.9736 - val_loss: 0.0889 - val_acc: 0.9704 - val_precision_1: 0.9316 - val_recall_1: 0.9955\n",
      "Epoch 2/2\n",
      "2313/2313 [==============================] - ETA: 0s - loss: 0.0151 - acc: 0.9958 - precision_1: 0.9963 - recall_1: 0.9925"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 23:23:13.780470: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 5898240000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: saving model to ./model/training_model/cp-0002\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3380d3d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3380d3d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33817250>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33817250>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3381c0d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3381c0d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33819f40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33819f40>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33822dc0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33822dc0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3382bc40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3382bc40>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33833af0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33833af0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3383b970>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3383b970>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337c37f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337c37f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337cc670>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337cc670>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337d44f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337d44f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337de370>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337de370>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/training_model/cp-0002/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/training_model/cp-0002/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2313/2313 [==============================] - 3167s 1s/step - loss: 0.0151 - acc: 0.9958 - precision_1: 0.9963 - recall_1: 0.9925 - val_loss: 0.0311 - val_acc: 0.9922 - val_precision_1: 0.9865 - val_recall_1: 0.9932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f9c30103940>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model.fit(x=[train_inputs, np.array(train_mask)], y=train_labels,\n",
    "          validation_data=([validation_inputs,np.array(validation_mask)], validation_labels),\n",
    "          batch_size=batch_size,epochs=2,verbose=1,\n",
    "          callbacks=[tensorboard_callback, cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e26c2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3380d3d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3380d3d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33817250>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33817250>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3381c0d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3381c0d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33819f40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33819f40>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33822dc0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33822dc0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3382bc40>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3382bc40>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33833af0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c33833af0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3383b970>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c3383b970>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337c37f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337c37f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337cc670>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337cc670>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337d44f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337d44f0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337de370>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7f9c337de370>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: xlnet-cnn-try-1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: xlnet-cnn-try-1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"xlnet-cnn-try-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a3d2c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)        [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " transformer (TFXLNetMainLa  TFXLNetModelOutput(last_hi   1167183   ['input_6[0][0]',             \n",
      " yer)                        dden_state=(None, 256, 768   36         'input_5[0][0]']             \n",
      "                             ),                                                                   \n",
      "                              mems=((256, None, 768),                                             \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768),                                                   \n",
      "                              (256, None, 768)),                                                  \n",
      "                              hidden_states=None, atten                                           \n",
      "                             tions=None)                                                          \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 125, 64)              344128    ['transformer[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 125, 64)              256       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 125, 64)              0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPoolin  (None, 62, 64)               0         ['activation_2[0][0]']        \n",
      " g1D)                                                                                             \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)         (None, 3968)                 0         ['max_pooling1d_1[0][0]']     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 1)                    3969      ['flatten_1[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 117066689 (446.57 MB)\n",
      "Trainable params: 117066561 (446.57 MB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f85e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 112s 753ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(x=[test_inputs,np.array(test_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec306f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f610928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './model/training_model/cp-0002'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d241682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 05:45:36.228232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.231882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.233095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.235562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.236791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.237983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.464528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.465788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.466891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-25 05:45:36.467972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12991 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:01:01.0, compute capability: 7.5\n",
      "/usr/local/lib/python3.8/dist-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer TruncatedNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "xlnetModel = tf.keras.models.load_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac4522c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/145 [..............................] - ETA: 7:11"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-25 05:46:47.097409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145/145 [==============================] - 112s 754ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = xlnetModel.predict(x=[test_inputs,np.array(test_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6cb6cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb660870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9632fd09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC Score: 0.999793645465137\n"
     ]
    }
   ],
   "source": [
    "auc_roc_score = roc_auc_score(test_labels,preds)\n",
    "print(\"AUC-ROC Score:\", auc_roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c93026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
