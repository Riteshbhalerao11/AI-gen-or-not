{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bb0c772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-24 06:23:20.729262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "18c7b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from pytorch_transformers import XLNetModel, XLNetTokenizer\n",
    "from transformers import XLNetForSequenceClassification\n",
    "from pytorch_transformers import AdamW\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83a3ec6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla T4'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d227a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Datasets/test_essays.csv')\n",
    "sub = pd.read_csv('Datasets/sample_submission.csv')\n",
    "org_train = pd.read_csv('Datasets/train_essays.csv')\n",
    "train = pd.read_csv(\"Datasets/train_v2_drcat_02.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3812340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop_duplicates(subset=['text'])\n",
    "train.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aee676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_train.rename(columns = {'generated':'label'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb4398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train[['text','label']], org_train[['text','label']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d808e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = train_data.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2948383",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence + \" [SEP] [CLS]\" for sentence in sentences]\n",
    "labels = train_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "809ccedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Phones\\n\\nModern humans today are always on their phone. They are always on their phone more than 5 hours a day no stop .All they do is text back and forward and just have group Chats on social media. They even do it while driving. They are some really bad consequences when stuff happens when it comes to a phone. Some certain areas in the United States ban phones from class rooms just because of it.\\n\\nWhen people have phones, they know about certain apps that they have .Apps like Facebook Twitter Instagram and Snapchat. So like if a friend moves away and you want to be in contact you can still be in contact by posting videos or text messages. People always have different ways how to communicate with a phone. Phones have changed due to our generation.\\n\\nDriving is one of the way how to get around. People always be on their phones while doing it. Which can cause serious Problems. That's why there's a thing that's called no texting while driving. That's a really important thing to remember. Some people still do it because they think It's stupid. No matter what they do they still have to obey it because that's the only way how did he save.\\n\\nSometimes on the news there is either an accident or a suicide. It might involve someone not looking where they're going or tweet that someone sent. It either injury or death. If a mysterious number says I'm going to kill you and they know where you live but you don't know the person's contact ,It makes you puzzled and make you start to freak out. Which can end up really badly.\\n\\nPhones are fine to use and it's also the best way to come over help. If you go through a problem and you can't find help you ,always have a phone there with you. Even though phones are used almost every day as long as you're safe it would come into use if you get into trouble. Make sure you do not be like this phone while you're in the middle of driving. The news always updated when people do something stupid around that involves their phones. The safest way is the best way to stay safe.     [SEP] [CLS]\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88b8d463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 798011/798011 [00:01<00:00, 662620.73B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁phones', '▁modern', '▁humans', '▁today', '▁are', '▁always', '▁on', '▁their', '▁phone', '.', '▁they', '▁are', '▁always', '▁on', '▁their', '▁phone', '▁more', '▁than', '▁5', '▁hours', '▁a', '▁day', '▁no', '▁stop', '▁', '.', 'all', '▁they', '▁do', '▁is', '▁text', '▁back', '▁and', '▁forward', '▁and', '▁just', '▁have', '▁group', '▁chat', 's', '▁on', '▁social', '▁media', '.', '▁they', '▁even', '▁do', '▁it', '▁while', '▁driving', '.', '▁they', '▁are', '▁some', '▁really', '▁bad', '▁consequences', '▁when', '▁stuff', '▁happens', '▁when', '▁it', '▁comes', '▁to', '▁a', '▁phone', '.', '▁some', '▁certain', '▁areas', '▁in', '▁the', '▁united', '▁states', '▁ban', '▁phones', '▁from', '▁class', '▁rooms', '▁just', '▁because', '▁of', '▁it', '.', '▁when', '▁people', '▁have', '▁phones', ',', '▁they', '▁know', '▁about', '▁certain', '▁apps', '▁that', '▁they', '▁have', '▁', '.', 'app', 's', '▁like', '▁face', 'book', '▁twitter', '▁in', 'sta', 'gram', '▁and', '▁snap', 'cha', 't', '.', '▁so', '▁like', '▁if', '▁a', '▁friend', '▁moves', '▁away', '▁and', '▁you', '▁want', '▁to', '▁be', '▁in', '▁contact', '▁you', '▁can', '▁still', '▁be', '▁in', '▁contact', '▁by', '▁posting', '▁videos', '▁or', '▁text', '▁messages', '.', '▁people', '▁always', '▁have', '▁different', '▁ways', '▁how', '▁to', '▁communicate', '▁with', '▁a', '▁phone', '.', '▁phones', '▁have', '▁changed', '▁due', '▁to', '▁our', '▁generation', '.', '▁driving', '▁is', '▁one', '▁of', '▁the', '▁way', '▁how', '▁to', '▁get', '▁around', '.', '▁people', '▁always', '▁be', '▁on', '▁their', '▁phones', '▁while', '▁doing', '▁it', '.', '▁which', '▁can', '▁cause', '▁serious', '▁problems', '.', '▁that', \"'\", 's', '▁why', '▁there', \"'\", 's', '▁a', '▁thing', '▁that', \"'\", 's', '▁called', '▁no', '▁text', 'ing', '▁while', '▁driving', '.', '▁that', \"'\", 's', '▁a', '▁really', '▁important', '▁thing', '▁to', '▁remember', '.', '▁some', '▁people', '▁still', '▁do', '▁it', '▁because', '▁they', '▁think', '▁it', \"'\", 's', '▁stupid', '.', '▁no', '▁matter', '▁what', '▁they', '▁do', '▁they', '▁still', '▁have', '▁to', '▁obey', '▁it', '▁because', '▁that', \"'\", 's', '▁the', '▁only', '▁way', '▁how', '▁did', '▁he', '▁save', '.', '▁sometimes', '▁on', '▁the', '▁news', '▁there', '▁is', '▁either', '▁an', '▁accident', '▁or', '▁a', '▁suicide', '.', '▁it', '▁might', '▁involve', '▁someone', '▁not', '▁looking', '▁where', '▁they', \"'\", 're', '▁going', '▁or', '▁tweet', '▁that', '▁someone', '▁sent', '.', '▁it', '▁either', '▁injury', '▁or', '▁death', '.', '▁if', '▁a', '▁mysterious', '▁number', '▁says', '▁', 'i', \"'\", 'm', '▁going', '▁to', '▁kill', '▁you', '▁and', '▁they', '▁know', '▁where', '▁you', '▁live', '▁but', '▁you', '▁don', \"'\", 't', '▁know', '▁the', '▁person', \"'\", 's', '▁contact', '▁', ',', 'it', '▁makes', '▁you', '▁puzzled', '▁and', '▁make', '▁you', '▁start', '▁to', '▁freak', '▁out', '.', '▁which', '▁can', '▁end', '▁up', '▁really', '▁badly', '.', '▁phones', '▁are', '▁fine', '▁to', '▁use', '▁and', '▁it', \"'\", 's', '▁also', '▁the', '▁best', '▁way', '▁to', '▁come', '▁over', '▁help', '.', '▁if', '▁you', '▁go', '▁through', '▁a', '▁problem', '▁and', '▁you', '▁can', \"'\", 't', '▁find', '▁help', '▁you', '▁', ',', 'al', 'ways', '▁have', '▁a', '▁phone', '▁there', '▁with', '▁you', '.', '▁even', '▁though', '▁phones', '▁are', '▁used', '▁almost', '▁every', '▁day', '▁as', '▁long', '▁as', '▁you', \"'\", 're', '▁safe', '▁it', '▁would', '▁come', '▁into', '▁use', '▁if', '▁you', '▁get', '▁into', '▁trouble', '.', '▁make', '▁sure', '▁you', '▁do', '▁not', '▁be', '▁like', '▁this', '▁phone', '▁while', '▁you', \"'\", 're', '▁in', '▁the', '▁middle', '▁of', '▁driving', '.', '▁the', '▁news', '▁always', '▁updated', '▁when', '▁people', '▁do', '▁something', '▁stupid', '▁around', '▁that', '▁involves', '▁their', '▁phones', '.', '▁the', '▁safest', '▁way', '▁is', '▁the', '▁best', '▁way', '▁to', '▁stay', '▁safe', '.', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "018f708c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc334f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab52c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91ade4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f25a57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=56, test_size=0.2)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=56, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5e38e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "876d61d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "899b58c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda824f932114e189ddfda01174ad08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286695541c444b59ab0c902e0eb4c137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XLNetForSequenceClassification(\n",
       "  (transformer): XLNetModel(\n",
       "    (word_embedding): Embedding(32000, 768)\n",
       "    (layer): ModuleList(\n",
       "      (0): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (4): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (5): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (6): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (7): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (8): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (9): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (10): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (11): XLNetLayer(\n",
       "        (rel_attn): XLNetRelativeAttention(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ff): XLNetFeedForward(\n",
       "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (activation_function): GELUActivation()\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (sequence_summary): SequenceSummary(\n",
       "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "    (first_dropout): Identity()\n",
       "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (logits_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2baf69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab5f5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                     lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "89c46be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 1/4 [15:22<46:08, 922.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.043495506795796154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [30:45<30:45, 922.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.014145305058825318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [46:08<15:22, 922.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.00981349447274758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [1:01:31<00:00, 922.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.007508797742750801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  \n",
    "  \n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "  \n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    loss = outputs[0]\n",
    "    logits = outputs[1]\n",
    "    train_loss_set.append(loss.item())    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fd332a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_XLNET-baseline.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f1117efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(validation_dataloader):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask, b_labels = batch\n",
    "      # Forward pass\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      # print (outputs)\n",
    "      prediction = torch.argmax(outputs[0],dim=1)\n",
    "      total += b_labels.size(0)\n",
    "      correct+=(prediction==b_labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a95801ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on vla data is: 99.28648648648648 %\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy of the model on vla data is: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cb3daf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('Datasets/test_essays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9cac227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Dress codes rulz! I mean, they're like, super ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>In recent years, there has been a growing move...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>Hey, so for this essay, I'm gonna talk about h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id                                               text\n",
       "0  0000aaaa          2  Dress codes rulz! I mean, they're like, super ...\n",
       "1  1111bbbb          3  In recent years, there has been a growing move...\n",
       "2  2222cccc          4  Hey, so for this essay, I'm gonna talk about h..."
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c20cde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = test.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aba47159",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [sentence + \" [SEP] [CLS]\" for sentence in test_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "588b483c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize the first sentence:\n",
      "['▁dress', '▁codes', '▁', 'r', 'ul', 'z', '!', '▁', 'i', '▁mean', ',', '▁they', \"'\", 're', '▁like', ',', '▁super', '▁important', '▁for', '▁creating', '▁a', '▁good', '▁learning', '▁environment', '.', '▁', 'i', '▁know', '▁some', '▁p', 'pl', '▁might', '▁think', '▁they', \"'\", 're', ',', '▁like', ',', '▁restrictive', '▁or', '▁whatever', ',', '▁but', '▁honestly', ',', '▁they', '▁help', '▁keep', '▁p', 'pl', '▁in', '▁line', '.', '▁first', '▁of', '▁all', ',', '▁dress', '▁codes', '▁promote', '▁equality', '.', '▁like', ',', '▁if', '▁everyone', \"'\", 's', '▁wearing', '▁the', '▁same', '▁thing', ',', '▁then', '▁no', '▁one', \"'\", 's', '▁gonna', '▁feel', '▁left', '▁out', '▁or', '▁judged', '.', '▁it', \"'\", 's', ',', '▁like', ',', '▁a', '▁way', '▁to', '▁eliminate', '▁bullying', '▁and', '▁stuff', '.', '▁and', '▁it', \"'\", 's', '▁not', '▁just', '▁about', '▁clothes', ',', '▁it', \"'\", 's', '▁about', ',', '▁like', ',', '▁respect', 'ing', '▁the', '▁school', '▁and', '▁the', '▁teachers', '.', '▁if', '▁we', '▁all', '▁dress', '▁the', '▁same', ',', '▁then', '▁we', \"'\", 're', ',', '▁like', ',', '▁showing', '▁that', '▁we', \"'\", 're', '▁all', '▁in', '▁this', '▁together', '.', '▁and', ',', '▁like', ',', '▁let', \"'\", 's', '▁be', '▁real', ',', '▁dress', '▁codes', '▁help', '▁us', '▁focus', '▁on', '▁learning', '.', '▁if', '▁we', \"'\", 're', '▁all', '▁dressed', '▁in', ',', '▁like', ',', '▁crazy', '▁clothes', ',', '▁then', '▁it', \"'\", 's', '▁gonna', '▁be', ',', '▁like', ',', '▁distract', 'ing', '.', '▁we', \"'\", 're', '▁gonna', '▁be', ',', '▁like', ',', '▁worried', '▁about', '▁what', '▁other', '▁p', 'pl', '▁think', '▁of', '▁us', '▁instead', '▁of', ',', '▁like', ',', '▁paying', '▁attention', '▁to', '▁the', '▁lesson', '.', '▁but', '▁if', '▁we', \"'\", 're', '▁all', '▁dressed', '▁the', '▁same', ',', '▁then', '▁it', \"'\", 's', ',', '▁like', ',', '▁easier', '▁to', '▁focus', '.', '▁but', ',', '▁like', ',', '▁the', '▁most', '▁important', '▁thing', '▁is', '▁that', '▁dress', '▁codes', '▁teach', '▁us', '▁about', ',', '▁like', ',', '▁responsibility', '▁and', '▁stuff', '.', '▁if', '▁we', '▁can', \"'\", 't', '▁even', '▁follow', '▁the', '▁dress', '▁code', ',', '▁then', '▁how', '▁are', '▁we', '▁gonna', ',', '▁like', ',', '▁succeed', '▁in', '▁life', '?', '▁it', \"'\", 's', ',', '▁like', ',', '▁a', '▁way', '▁for', '▁us', '▁to', '▁learn', '▁how', '▁to', ',', '▁like', ',', '▁follow', '▁rules', '▁and', '▁stuff', '.', '▁and', '▁it', \"'\", 's', '▁not', '▁just', '▁about', '▁school', ',', '▁it', \"'\", 's', '▁about', ',', '▁like', ',', '▁life', '▁in', '▁general', '.', '▁so', ',', '▁like', ',', '▁in', '▁conclusion', ',', '▁dress', '▁codes', '▁are', ',', '▁like', ',', '▁super', '▁beneficial', '▁to', '▁the', '▁school', '▁environment', '.', '▁they', '▁promote', '▁equality', ',', '▁focus', ',', '▁and', '▁responsibility', '.', '▁and', ',', '▁like', ',', '▁honestly', ',', '▁who', '▁doesn', \"'\", 't', '▁want', '▁those', '▁things', '?', '▁so', ',', '▁like', ',', '▁let', \"'\", 's', '▁all', '▁just', ',', '▁like', ',', '▁follow', '▁the', '▁dress', '▁code', '▁and', ',', '▁like', ',', '▁be', '▁good', '▁students', ',', '▁okay', '?', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "test_tokenized_texts = [tokenizer.tokenize(sent) for sent in test_sentences]\n",
    "print (\"Tokenize the first sentence:\")\n",
    "print (test_tokenized_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d40615c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in test_tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c7886c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4f2c3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "test_attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in test_input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  test_attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "63ebf09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_masks = torch.tensor(test_attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "043567ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TensorDataset(test_inputs, test_masks)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5328e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, batch in enumerate(test_dataloader):\n",
    "      batch = tuple(t.to(device) for t in batch)\n",
    "      # Unpack the inputs from our dataloader\n",
    "      b_input_ids, b_input_mask = batch\n",
    "      # Forward pass\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      # print (outputs)\n",
    "      prediction = torch.argmax(outputs[0],dim=1)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ca0e0267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9939180612564087, 0.995185911655426, 0.9943175911903381]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "32e36bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.sigmoid(outputs[0])[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fa68ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = {'id':test.id.values,'generated':preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9d899758",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = pd.DataFrame(sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a14c3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions.to_csv(\"Submissions.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf18d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
